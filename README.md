# Ask MaaS - Pure RAG System

A production-ready Retrieval-Augmented Generation (RAG) system for Red Hat Developer articles, implementing pure vector-based semantic search with global context awareness.

## ğŸš€ Features

- **Pure Vector Search**: Uses ONLY embeddings and cosine similarity (no keyword matching)
- **Global Context Search**: Every query searches across ALL indexed articles
- **Fresh Retrieval**: Each query performs fresh context retrieval without caching
- **Streaming Responses**: Real-time SSE streaming for chat responses
- **Production Ready**: Deployed on OpenShift with Redis caching and horizontal scaling

## ğŸ“ Project Structure

```
ask-maas/
â”œâ”€â”€ ask-maas-api/              # Backend API service
â”‚   â”œâ”€â”€ app/                   # Core application
â”‚   â”‚   â”œâ”€â”€ routers/          # API endpoints
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py       # Chat endpoint with pure RAG
â”‚   â”‚   â”‚   â””â”€â”€ ingest.py     # Document ingestion
â”‚   â”‚   â”œâ”€â”€ services/         # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_retrieval.py  # Pure vector search implementation
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py        # LLM integration
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py      # Redis caching
â”‚   â”‚   â”‚   â””â”€â”€ config.py     # Configuration
â”‚   â”‚   â””â”€â”€ models/           # Data models
â”‚   â”œâ”€â”€ Dockerfile            # Container definition
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â””â”€â”€ ingest.py            # Article ingestion script
â”œâ”€â”€ ghost-site/               # Frontend application
â”‚   â”œâ”€â”€ src/                  # React/Next.js source
â”‚   â”œâ”€â”€ Dockerfile           # Frontend container
â”‚   â””â”€â”€ package.json         # Node dependencies
â”œâ”€â”€ k8s/                     # Kubernetes/OpenShift configs
â”‚   â”œâ”€â”€ api/                # API deployment configs
â”‚   â”œâ”€â”€ models/             # Model service configs
â”‚   â””â”€â”€ namespaces/         # Namespace definitions
â”œâ”€â”€ articles/               # Sample articles for testing
â”œâ”€â”€ deploy-ask-maas.sh     # Deployment script
â””â”€â”€ IMPROVEMENTS.md        # Technical improvements documentation
```

## ğŸ› ï¸ Technology Stack

- **Backend**: FastAPI, Python 3.11
- **Vector Search**: FAISS with cosine similarity
- **LLM**: vLLM with Mistral-7B
- **Embeddings**: Text Embeddings Inference (TEI)
- **Cache**: Redis
- **Frontend**: Next.js, React
- **Deployment**: OpenShift/Kubernetes

## ğŸ“¦ Installation

### Prerequisites
- Python 3.11+
- Docker/Podman
- OpenShift CLI (oc) or kubectl
- Access to OpenShift cluster

### Local Development

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/ask-maas.git
cd ask-maas
```

2. **Setup Python environment**
```bash
cd ask-maas-api
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

3. **Configure environment**
```bash
cp example-env .env
# Edit .env with your configuration
```

4. **Start Redis locally**
```bash
docker run -d -p 6379:6379 redis:latest
```

5. **Run the API**
```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

## ğŸš€ Deployment

### Build and Push Image
```bash
cd ask-maas-api
podman build -t ask-maas-api:latest .
podman tag ask-maas-api:latest your-registry/ask-maas-api:latest
podman push your-registry/ask-maas-api:latest
```

### Deploy to OpenShift
```bash
# Deploy using the provided script
./deploy-ask-maas.sh

# Or manually
oc apply -f k8s/namespaces/
oc apply -f k8s/api/
oc apply -f k8s/models/
```

### Ingest Articles
```bash
cd ask-maas-api
python ingest.py
```

## ğŸ“Š API Endpoints

### Health Check
```bash
GET /health
```

### Chat (Pure RAG)
```bash
POST /api/v1/chat
{
  "query": "What is MaaS?",
  "page_url": "any-url",
  "stream": true
}
```

### Ingest Content
```bash
POST /api/v1/ingest/content
{
  "page_url": "https://example.com/article",
  "title": "Article Title",
  "content": "Article content...",
  "content_type": "text",
  "force_refresh": true
}
```

## ğŸ”‘ Key Improvements (Pure RAG)

This system implements a **pure RAG approach**:

1. **No Keyword Matching**: Completely removed BM25 and lexical search
2. **Pure Vector Search**: Uses only embeddings and cosine similarity
3. **Global Context**: Every query searches across ALL indexed articles
4. **Fresh Retrieval**: No query result caching, fresh search for each request
5. **Optimized Performance**: Batch embedding generation, reduced chunk sizes

See [IMPROVEMENTS.md](IMPROVEMENTS.md) for detailed technical changes.

## ğŸ§ª Testing

### Test a Query
```python
import requests

response = requests.post(
    "http://localhost:8000/api/v1/chat",
    json={
        "query": "What is MaaS?",
        "page_url": "test",
        "stream": False
    }
)
print(response.json())
```

### Expected Queries That Work
- "What is MaaS?" â†’ Returns MaaS definition
- "How does vLLM compare to Ollama?" â†’ Returns comparison
- "What is TTFT and ITL?" â†’ Returns metrics definitions
- "How to deploy Llama 3 with vLLM?" â†’ Returns deployment guide

## ğŸ”§ Configuration

Key environment variables in `.env`:

```env
# Model Services
VLLM_URL=http://vllm-service:8080
TEI_EMBEDDINGS_URL=http://tei-embeddings:8080
TEI_RERANKER_URL=http://tei-reranker:8080

# Redis Cache
REDIS_HOST=redis-service
REDIS_PORT=6379

# Retrieval Settings
RETRIEVAL_TOP_K=20          # Reduced for performance
MIN_SIMILARITY_SCORE=0.1    # Minimum similarity threshold
CHUNK_SIZE=800              # Optimized chunk size
```

## ğŸ“ˆ Performance

- **Query Success Rate**: 83% (5/6 test queries)
- **Average Response Time**: ~30 seconds (can be optimized with GPU)
- **Global Context**: âœ… Working across all articles
- **Pure Vector Search**: âœ… No keyword matching

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add feature'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create Pull Request

## ğŸ“ License

[Your License]

## ğŸ†˜ Support

For issues and questions:
- Open an issue on GitHub
- Check [IMPROVEMENTS.md](IMPROVEMENTS.md) for technical details
- Review the deployment logs for troubleshooting

## ğŸ—ï¸ Architecture

```
User Query
    â†“
Vector Embedding Generation (Fresh)
    â†“
Global Search Across All Articles
    â†“
Cosine Similarity Scoring
    â†“
Result Diversification (Max 3 per article)
    â†“
Optional Reranking
    â†“
LLM Response Generation
    â†“
SSE Streaming to User
```

## ğŸš¦ System Status

- **Production URL**: Configure in deployment
- **Health Endpoint**: `/health`
- **Metrics**: Prometheus-compatible `/metrics`
- **Logs**: Structured JSON logging with correlation IDs

---
*Built with â¤ï¸ for pure semantic search*