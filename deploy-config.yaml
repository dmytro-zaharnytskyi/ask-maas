# Ask MaaS Deployment Configuration
# This file contains configuration options for the deployment script

# Model Configuration
model:
  # Available options: mistral, llama3, mixtral, qwen
  type: mistral
  
  # Model-specific settings
  mistral:
    name: "mistralai/Mistral-7B-Instruct-v0.3"
    replicas: 1
    gpu_count: 1
    cpu_request: "2"
    memory_request: "16Gi"
    memory_limit: "24Gi"
    gpu_memory: "16GB"
  
  llama3:
    name: "meta-llama/Meta-Llama-3-8B-Instruct"
    replicas: 1
    gpu_count: 1
    cpu_request: "2"
    memory_request: "16Gi"
    memory_limit: "24Gi"
    gpu_memory: "16GB"
  
  mixtral:
    name: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    replicas: 1
    gpu_count: 2
    cpu_request: "4"
    memory_request: "48Gi"
    memory_limit: "64Gi"
    gpu_memory: "48GB"
  
  qwen:
    name: "Qwen/Qwen2.5-32B-Instruct"
    replicas: 1
    gpu_count: 2
    cpu_request: "8"
    memory_request: "64Gi"
    memory_limit: "96Gi"
    gpu_memory: "64GB"

# Embedding Service Configuration
embeddings:
  model: "BAAI/bge-base-en-v1.5"
  replicas: 1
  cpu_request: "1"
  memory_request: "2Gi"
  memory_limit: "4Gi"

# Reranker Service Configuration
reranker:
  enabled: true
  model: "BAAI/bge-reranker-base"
  replicas: 1
  cpu_request: "1"
  memory_request: "2Gi"
  memory_limit: "4Gi"

# Vector Database Configuration
qdrant:
  replicas: 1
  cpu_request: "500m"
  memory_request: "1Gi"
  memory_limit: "2Gi"
  storage_size: "10Gi"
  collection_name: "ask-maas"
  vector_size: 768
  distance: "Cosine"

# Redis Cache Configuration
redis:
  replicas: 1
  cpu_request: "100m"
  memory_request: "256Mi"
  memory_limit: "512Mi"
  maxmemory_policy: "allkeys-lru"

# Orchestrator API Configuration
orchestrator:
  replicas: 2
  cpu_request: "500m"
  memory_request: "1Gi"
  memory_limit: "2Gi"
  
  # Environment variables
  env:
    LOG_LEVEL: "INFO"
    REQUEST_TIMEOUT: "60"
    CORS_ORIGINS: '["*"]'
    
  # RAG settings
  retrieval:
    top_k: 20
    min_similarity: 0.1
    chunk_size: 800
    chunk_overlap: 200
    max_chunks_per_article: 3
    use_reranker: false

# Frontend Configuration
frontend:
  replicas: 2
  cpu_request: "100m"
  memory_request: "256Mi"
  memory_limit: "512Mi"
  node_env: "production"

# Ingestion Configuration
ingestion:
  # Articles to ingest (relative to articles/ directory)
  articles:
    - filename: "All you can kustomize during the MaaS deployment _ Red Hat Developer.html"
      title: "All you can kustomize during the MaaS deployment"
    - filename: "What is MaaS (Models-as-a-Service) and how to set it up fast on OpenShift _ Red Hat Developer.html"
      title: "What is MaaS (Models-as-a-Service) and how to set it up fast on OpenShift"
    - filename: "Deploy Llama 3 8B with vLLM _ Red Hat Developer.html"
      title: "Deploy Llama 3 8B with vLLM"
    - filename: "Ollama vs. vLLM_ A deep dive into performance benchmarking _ Red Hat Developer.html"
      title: "Ollama vs. vLLM: A deep dive into performance benchmarking"
    - filename: "Profiling vLLM Inference Server with GPU acceleration on RHEL _ Red Hat Developer.html"
      title: "Profiling vLLM Inference Server with GPU acceleration on RHEL"
  
  # Ingestion settings
  max_content_length: 50000
  batch_size: 5
  retry_attempts: 3

# Monitoring Configuration
monitoring:
  prometheus:
    enabled: false
    scrape_interval: "30s"
  
  grafana:
    enabled: false
    admin_password: "admin"
  
  metrics:
    enabled: true
    port: 9090

# Network Configuration
network:
  # Route configuration
  routes:
    api_subdomain: "ask-maas-api"
    frontend_subdomain: "ask-maas-frontend"
    tls_enabled: true
  
  # Service mesh (optional)
  service_mesh:
    enabled: false
    istio_version: "1.18"

# Security Configuration
security:
  # RBAC
  rbac:
    enabled: true
    service_account: "ask-maas-sa"
  
  # Network policies
  network_policies:
    enabled: false
    
  # Pod security
  pod_security:
    run_as_non_root: true
    fs_group: 1001
    
# Backup Configuration
backup:
  enabled: false
  schedule: "0 2 * * *"  # Daily at 2 AM
  retention_days: 7

# Advanced Configuration
advanced:
  # Node selectors
  node_selector:
    gpu_nodes:
      nvidia.com/gpu.present: "true"
    cpu_nodes: {}
  
  # Tolerations for GPU nodes
  gpu_tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  
  # Resource quotas
  resource_quotas:
    enabled: false
    max_cpu: "100"
    max_memory: "200Gi"
    max_storage: "500Gi"